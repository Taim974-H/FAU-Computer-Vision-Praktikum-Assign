for evaluation.py:

###################################
# notes - some theory for better understanding

# similarities: "Are these faces similar enough to be considered the same person?"
# Similarity Score: 0.8 (for example)
# If threshold = 0.7: Accept as match (0.8 â‰¥ 0.7)
# If threshold = 0.9: Reject as match (0.8 < 0.9)

# LOW threshold:

# More matches accepted (both correct and incorrect)
# Higher false alarm rate (accepting unknown people)
# Higher identification rate (correctly identifying known people)


# HIGH threshold:

# Fewer matches accepted
# Lower false alarm rate (rejecting unknown people)
# Lower identification rate (might reject known people)

# example
# Unknown people's similarity scores: [0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]
# If we want 10% false alarm rate:
# - We need a threshold where only 10% of unknown people get scores above it
# - percentile = (1 - 0.1) * 100 = 90
# - This means we want the threshold where 90% of scores are below it
# - If we set threshold = 0.85, then only one score (1.0) is above it
# - This gives us roughly our desired 10% false alarm rate

###################################

# We have:
# - test_labels (containing both known and unknown subjects)
# - test_embeddings (face features for all test subjects)
# - false_alarm_rate_range (e.g., [0.001, 0.002, ..., 1.0])
# - trained classifier that gives us predictions and similarity scores

# For each false alarm rate in our range, we:
# a) First get similarity scores and initial predictions
# b) Then find the threshold that would give us that false alarm rate
#     # Look at similarities of UNKNOWN subjects only..
#     # If we want FAR = 0.1 (10%):
#     # We need a threshold where 10% of unknown subjects get wrongly accepted
#     # So we find the similarity value where 90% of unknown similarities fall below it
# c) Use this threshold to correct our predictions
#     # If similarity < threshold: predict as UNKNOWN
#     # If similarity >= threshold: keep original prediction
# d) Calculate identification rate using corrected predictions

# # For each false_alarm_rate, we get:
# - A threshold that would give us that FAR
# - An identification rate (DIR) achieved at that threshold

# # This creates our curve:
# X-axis: false alarm rates we tested
# Y-axis: identification rates we achieved at each FAR
